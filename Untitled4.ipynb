{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0WOuLGC4A6cn",
    "outputId": "5deb9740-320b-4cc9-9e2e-8d7092386b79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting segmentation-models-pytorch\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/54/8953f9f7ee9d451b0f3be8d635aa3a654579abf898d17502a090efe1155a/segmentation_models_pytorch-0.1.3-py3-none-any.whl (66kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 2.9MB/s \n",
      "\u001b[?25hCollecting timm==0.3.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/2d/39ecc56fbb202e1891c317e8e44667299bc3b0762ea2ed6aaaa2c2f6613c/timm-0.3.2-py3-none-any.whl (244kB)\n",
      "\u001b[K     |████████████████████████████████| 245kB 4.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch) (0.10.0+cu102)\n",
      "Collecting efficientnet-pytorch==0.6.3\n",
      "  Downloading https://files.pythonhosted.org/packages/b8/cb/0309a6e3d404862ae4bc017f89645cf150ac94c14c88ef81d215c8e52925/efficientnet_pytorch-0.6.3.tar.gz\n",
      "Collecting pretrainedmodels==0.7.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/0e/be6a0e58447ac16c938799d49bfb5fb7a80ac35e137547fc6cee2c08c4cf/pretrainedmodels-0.7.4.tar.gz (58kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 4.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch>=1.0 in /usr/local/lib/python3.7/dist-packages (from timm==0.3.2->segmentation-models-pytorch) (1.9.0+cu102)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.3.0->segmentation-models-pytorch) (1.19.5)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.3.0->segmentation-models-pytorch) (7.1.2)\n",
      "Collecting munch\n",
      "  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch) (4.41.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0->timm==0.3.2->segmentation-models-pytorch) (3.7.4.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch) (1.15.0)\n",
      "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-cp37-none-any.whl size=12420 sha256=16ce465fd3dce9ea679aaef8fd3765f34480f60595ee9784455733140fbda8d0\n",
      "  Stored in directory: /root/.cache/pip/wheels/42/1e/a9/2a578ba9ad04e776e80bf0f70d8a7f4c29ec0718b92d8f6ccd\n",
      "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-cp37-none-any.whl size=60966 sha256=56749c58c5054495f297c4fe29030cd26ef9188f244d579fb386e8f74d80ab45\n",
      "  Stored in directory: /root/.cache/pip/wheels/69/df/63/62583c096289713f22db605aa2334de5b591d59861a02c2ecd\n",
      "Successfully built efficientnet-pytorch pretrainedmodels\n",
      "Installing collected packages: timm, efficientnet-pytorch, munch, pretrainedmodels, segmentation-models-pytorch\n",
      "Successfully installed efficientnet-pytorch-0.6.3 munch-2.5.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.1.3 timm-0.3.2\n"
     ]
    }
   ],
   "source": [
    " !pip install segmentation-models-pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BN1UMxdUcf1h"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "import cv2\n",
    "import os\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch import optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A07GFx-Ecn1_",
    "outputId": "99933343-ac7e-4130-f826-ecf984144614"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-XpAlaClz5jw"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/HT/histology_dataset.zip\", 'r')\n",
    "zip_ref.extractall(\"/content/drive/MyDrive/HT\")\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kn-tNRiac4vJ"
   },
   "outputs": [],
   "source": [
    "class histologyDataset(Dataset):\n",
    "    def __init__(self, imgs_dir, gt_dir, augs=None):\n",
    "        self.imgs_dir = imgs_dir\n",
    "        self.masks_dir = gt_dir\n",
    "        self.num_classes = len(os.listdir(gt_dir))\n",
    "        self.gt_classes = os.listdir(gt_dir)\n",
    "        self.gt_classes.sort()\n",
    "        self.im_names = os.listdir(self.imgs_dir)\n",
    "        self.augs = augs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.im_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        im_name = self.im_names[idx]\n",
    "        gt_names = [os.path.join(self.masks_dir, gt_class, im_name) for gt_class in self.gt_classes]\n",
    "        \n",
    "        img = np.expand_dims(plt.imread(os.path.join(self.imgs_dir, im_name), 0), axis=0)\n",
    "        mask = np.array([plt.imread(i, 0) for i in gt_names])\n",
    "\n",
    "        return {\n",
    "            'image': torch.from_numpy(img).type(torch.FloatTensor),\n",
    "            'mask': torch.from_numpy(mask).type(torch.FloatTensor)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "pxD3rvSPc6yN",
    "outputId": "d1a9d97d-513c-4b2a-db1b-e802fe316eb2"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c84512ddcccf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistologyDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./histology_dataset/train/images/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./histology_dataset/train/GT/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b071ef6541db>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, imgs_dir, gt_dir, augs)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasks_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgt_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgt_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './histology_dataset/train/GT/'"
     ]
    }
   ],
   "source": [
    "dataset = histologyDataset(\"./histology_dataset/train/images/\", \"./histology_dataset/train/GT/\")\n",
    "plt.imshow(dataset[0]['mask'].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7bgTI3YHq22"
   },
   "outputs": [],
   "source": [
    " import segmentation_models_pytorch as smp\n",
    " #import collections.abc as container_abcs\n",
    "\n",
    "model = smp.FPN(\n",
    "    encoder_name=\"resnet34\",       \n",
    "    encoder_weights=\"imagenet\",     \n",
    "    in_channels=1,                  \n",
    "    classes=9,                      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLIaeft6KCGC"
   },
   "outputs": [],
   "source": [
    "from segmentation_models_pytorch.encoders import get_preprocessing_fn\n",
    "\n",
    "preprocess_input = get_preprocessing_fn('resnet18', pretrained='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ih1i4aNrPss4"
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "from segmentation_models_pytorch.fpn.decoder import FPNDecoder\n",
    "from segmentation_models_pytorch.base import SegmentationModel, SegmentationHead, ClassificationHead\n",
    "from segmentation_models_pytorch.encoders import get_encoder\n",
    "\n",
    "\n",
    "class FPN(SegmentationModel):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str = \"resnet34\",\n",
    "        encoder_depth: int = 5,\n",
    "        encoder_weights: Optional[str] = \"imagenet\",\n",
    "        decoder_pyramid_channels: int = 256,\n",
    "        decoder_segmentation_channels: int = 128,\n",
    "        decoder_merge_policy: str = \"add\",\n",
    "        decoder_dropout: float = 0.2,\n",
    "        in_channels: int = 1,\n",
    "        classes: int = 9,\n",
    "        activation: Optional[str] = \"softmax\",\n",
    "        upsampling: int = 4,\n",
    "        aux_params: Optional[dict] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = get_encoder(\n",
    "            encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            depth=encoder_depth,\n",
    "            weights=encoder_weights,\n",
    "        )\n",
    "\n",
    "        self.decoder = FPNDecoder(\n",
    "            encoder_channels=self.encoder.out_channels,\n",
    "            encoder_depth=encoder_depth,\n",
    "            pyramid_channels=decoder_pyramid_channels,\n",
    "            segmentation_channels=decoder_segmentation_channels,\n",
    "            dropout=decoder_dropout,\n",
    "            merge_policy=decoder_merge_policy,\n",
    "        )\n",
    "\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=self.decoder.out_channels,\n",
    "            out_channels=classes,\n",
    "            activation=activation,\n",
    "            kernel_size=1,\n",
    "            upsampling=upsampling,\n",
    "        )\n",
    "\n",
    "        if aux_params is not None:\n",
    "            self.classification_head = ClassificationHead(\n",
    "                in_channels=self.encoder.out_channels[-1], **aux_params\n",
    "            )\n",
    "        else:\n",
    "            self.classification_head = None\n",
    "\n",
    "        self.name = \"fpn-{}\".format(encoder_name)\n",
    "        self.initialize()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "id": "WW23qQQfETUc",
    "outputId": "1f5953d4-1da9-4343-cf96-8aff9c499d02"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e126e6a2c7af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mLR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistologyDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./histology_dataset/train/images/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./histology_dataset/train/GT/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mn_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mVAL_PERCENT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b071ef6541db>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, imgs_dir, gt_dir, augs)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasks_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgt_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgt_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './histology_dataset/train/GT/'"
     ]
    }
   ],
   "source": [
    "VAL_PERCENT = 0.1\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 16\n",
    "LR = 0.1\n",
    "\n",
    "dataset = histologyDataset(\"./histology_dataset/train/images/\", \"./histology_dataset/train/GT/\")\n",
    "n_val = int(len(dataset) * VAL_PERCENT)\n",
    "n_train = len(dataset) - n_val\n",
    "train_set, val_set = random_split(dataset, [n_train, n_val])\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=8, pin_memory=True, drop_last=True)\n",
    "\n",
    "model = FPN()\n",
    "criterion = nn.DiceLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=LR, weight_decay=1e-8, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    print(f\"{epoch}/{EPOCHS}\")\n",
    "    for batch in tqdm(train_loader):\n",
    "        imgs = batch['image']\n",
    "        true_masks = batch['mask']\n",
    "        pred_mask = model(imgs)\n",
    "        loss = criterion(pred_mask, true_masks)\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_value_(model.parameters(), 0.1)\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
